{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework #0 - CSE 572 Fall '23\n",
    "### Tyler Fichiera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provided code for tokenizing the article text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# stemming tool from nltk\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# a mapping dictionary that help remove punctuations\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "\n",
    "def get_tokens(text):\n",
    "  # turn document into lowercase\n",
    "  lowers = text.lower()\n",
    "  # remove punctuations\n",
    "  no_punctuation = lowers.translate(remove_punctuation_map)\n",
    "  # tokenize document\n",
    "  tokens = nltk.word_tokenize(no_punctuation)\n",
    "  # remove stop words\n",
    "  filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "  # stemming process\n",
    "  stemmed = []\n",
    "  for item in filtered:\n",
    "    stemmed.append(stemmer.stem(item))\n",
    "  # final unigrams\n",
    "  return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dictionary from the given text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary.txt', 'r') as dictionary_file:\n",
    "    dictionary = [line.strip() for line in dictionary_file.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load news articles from given csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, math, json\n",
    "from collections import defaultdict\n",
    "\n",
    "article_categories = {}\n",
    "\n",
    "unigram_occurrences_by_article = {}\n",
    "\n",
    "dictionary_word_article_occcurences = defaultdict(int)\n",
    "\n",
    "N = 0\n",
    "\n",
    "with open('news-train-1.csv', 'r') as csv_file:\n",
    "  csv_reader = csv.reader(csv_file)\n",
    "\n",
    "  next(csv_reader) # skip header row\n",
    "\n",
    "  for article_id, article_text, article_category in csv_reader:\n",
    "    N += 1\n",
    "\n",
    "    article_unigrams = get_tokens(article_text)\n",
    "    article_categories[article_id] = article_category\n",
    "\n",
    "    unigram_occurences = {}\n",
    "    for word in dictionary:\n",
    "      num_of_occurences = article_unigrams.count(word)\n",
    "\n",
    "      if num_of_occurences > 0:\n",
    "        dictionary_word_article_occcurences[word] += 1\n",
    "\n",
    "      unigram_occurences[word] = num_of_occurences\n",
    "\n",
    "    unigram_occurrences_by_article[article_id] = unigram_occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate desired data and output to txt/json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_word_frequencies = defaultdict(lambda: defaultdict(int))\n",
    "category_word_tfidf_scores = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "TFIDF_matrix = []\n",
    "\n",
    "for article_id in unigram_occurrences_by_article:\n",
    "  matrix_row = []\n",
    "\n",
    "  article_unigrams = unigram_occurrences_by_article[article_id]\n",
    "  max_occurences = max(unigram_occurrences_by_article[article_id].values())\n",
    "\n",
    "  for word in dictionary:\n",
    "    TF_ij = article_unigrams[word] / max_occurences\n",
    "    IDF_j = math.log(N / dictionary_word_article_occcurences[word])\n",
    "    TFIDF_ij = TF_ij * IDF_j\n",
    "    \n",
    "    matrix_row.append(TFIDF_ij)\n",
    "\n",
    "    article_category = article_categories[article_id]\n",
    "    category_word_tfidf_scores[article_category][word].append(TFIDF_ij)\n",
    "    category_word_frequencies[article_category][word] += article_unigrams[word]\n",
    "\n",
    "  TFIDF_matrix.append(matrix_row)\n",
    "\n",
    "with open('matrix.txt', 'w') as file:\n",
    "  for row in TFIDF_matrix:\n",
    "    row_str = ','.join(map(str, row))\n",
    "    file.write(row_str + '\\n')\n",
    "\n",
    "top_words_by_category_frequency = {}\n",
    "top_words_by_category_scores = {}\n",
    "\n",
    "for category, word_frequencies in category_word_frequencies.items():\n",
    "  sorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "  top_words_frequency = {word: frequency for word, frequency in sorted_words[:3]}\n",
    "  top_words_by_category_frequency[category] = top_words_frequency\n",
    "\n",
    "for category, word_tfidf_scores in category_word_tfidf_scores.items():\n",
    "  sorted_words = sorted(word_tfidf_scores.items(), key=lambda x: sum(x[1]) / len(x[1]), reverse=True)\n",
    "  top_words_scores = {word: sum(scores) / len(scores) for word, scores in sorted_words[:3]}\n",
    "  top_words_by_category_scores[category] = top_words_scores\n",
    "\n",
    "with open('frequency.json', 'w') as json_file:\n",
    "  json.dump(top_words_by_category_frequency, json_file, indent=4)\n",
    "\n",
    "with open('scores.json', 'w') as json_file:\n",
    "  json.dump(top_words_by_category_scores, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
